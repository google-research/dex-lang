
' # Mini-PPL for Discrete variables in Dex

' This document is an example for creating a small PPL in Dex. This is
inspired by PPL's written for Haskell that utilize monadic
represtations of the chain rule for representing discrete
variables. Notably though in Dex we aim for a non-monadic style that
primarily uses tables and autodifferentiation. 


' ### General Helper functions for printing

' These are some general functions to make printing easier
 throughout the document. 

def join (joiner: List a) (lists:n=>(List a)) : List a =
    -- Join together lists with an intermediary joiner
    concat $ for i.
        case ordinal i == (size n - 1) of
             True -> lists.i
             False -> lists.i <> joiner

instance Show (Fin a)
  show = \ d. show $ ordinal d

instance [Show a, Show b] Show (a & b)
  show = \(a, b). "(" <> show a <> ", " <> show b <> ")"

instance [Mul a, Mul b] Mul (a & b)
     mul = \ (a, b) (a', b').  (a * a', b * b')
     one = (one, one)


' ## Distributions

' A simple representation of a (possibly unnormalized) probability
  table. As well as a alias for a conditional distribution.
  Probs are defined as `data` to allow for sparse representations.

Binary = Fin 2

data Dist output cond =
    AsDist (cond => output => Float)
def UDist (output:Type) : Type = Dist output (Fin 1)

def unsafeDist (m:Type) (AsDist xs: Dist n c) : (Dist m c) =
    AsDist for i. unsafeCastTable m xs.i
    
def unsafeCond (m:Type) (AsDist xs: Dist o c) : (Dist o m) =
    AsDist $ unsafeCastTable m xs

def bernoulli (p: n=>Float) : Dist m n =  unsafeDist m $ AsDist for i.  [1.0 - p.i, p.i]
def delta (x : n=>m) : Dist m n  = AsDist for i. yieldState zero $ \ aref. aref!(x.i) := 1.0
def uniform : Dist m n  = AsDist  for j. for i. 1.0 / (IToF (size m))


' Assert Conditional Independence
$$p(a,\ | \ b,\ c) = p(a| \ b)$$

def ind (AsDist dist : Dist a b) : Dist a (b & c) =
    AsDist for (j,k). dist.j


' Assert Conditional Independence rule
$$p(a,\ b\ | \ c) = p(a\ |\ c)\ p(b\ |\ c )$$

def (,,) (AsDist a_dist : Dist a c) (AsDist b_dist : Dist b c) : Dist (a & b) c =
    AsDist for k. for (i, j). a_dist.k.i * b_dist.k.j



' Apply Chain rule
$$p(a,\ b\ |\ c) = p(a\ |\ b,\ c)\ p(b\ |\ c)$$

def chain (AsDist a_dist : Dist a (b & c)) (AsDist b_dist : Dist b c) : Dist (a & b) c =
    AsDist $ for k : c.
        for (j, i).
           a_dist.(i,k).j * b_dist.k.i
        

' Apply Markov rule
$$p(a\ |\ c) = \sum_b p(a\ |\ b)\ p(b\ |\ c)$$

def (<|) (AsDist a_dist : Dist a b) (AsDist b_dist : Dist b c) : Dist a c =
    AsDist $ for k : c.
        for j: a. sum for i: b.
           a_dist.i.j * b_dist.k.i

[0.0, 1.0] : {sprinkler : Fin 2} => Float
:t for (s, r). [[0.0, 0.8], [0.9, 0.99]].s.r

[0.0, 0.8, 0.9, 0.99] : ((Fin 4 & Fin 1) => Float32)

Rain = {rain: Binary}
Sprinkler = {sprinkler: Binary}
Wet = {wet: Binary}

def wet : UDist Wet =
    -- Rain is drawn from a bernoulli
    rain : UDist Rain = bernoulli [0.2]
    sprinkler : Dist Sprinkler Rain = bernoulli ([0.4, 0.01]:Rain => Float)

    -- Sprinkler is drawn conditional on rain
    sprinkler_rain : UDist (Sprinkler & Rain)  = ind sprinkler `chain` rain

    -- CPT for producing wet grass conditioned on other variables
    grass : UDist (Sprinkler & Rain) = bernoulli for (s, r):(Sprinkler & Rain). [[0.0, 0.8], [0.9, 0.99]].s.r
                         
    -- Grass is drawn conditional on Rain / Sprinkler
    grass <| sprinkler_rain



' The model is a simple bayes' net over three latent variables
  Rain (R), Sprinkler (S), and Wet grass (G).

def wet (rain_var:DVar Binary) (sprinkler_var:DVar Binary) (wet_var:DVar Binary):
        Probs Binary =
    -- Rain is drawn from a bernoulli
    rain  = sample rain_var $ bernoulli 0.2

    -- Sprinkler is drawn conditional on rain. It is non-Markovian.
    sprinkler_rain = cond_sample sprinkler_var [bernoulli 0.4, bernoulli 0.01] rain

    -- CPT for producing wet grass conditioned on other variables
    trans = for (s, r). [[bernoulli 0.0, bernoulli 0.8],
                         [bernoulli 0.9, bernoulli 0.99]].s.r
                         
    -- Grass is drawn conditional on Rain / Sprinkler
    sample wet_var $ trans <| sprinkler_rain




-- ' Observe a variable

-- def obs (x: b) (zdist : Probs a) (x_z : CDist b a) : Probs a =
--     (AsProbsTable z) = zdist
--     AsProbsTable $ for i.
--         (AsProbsTable x') = x_z.i
--         z.i .* x'.x


data Probs a = AsProbsTable (a => Float)
def CDist (a:Type) (c:Type): Type =  c => Probs a

instance [Show a] Show (Probs a)
  show = \ d.
       case d of 
         AsProbsTable d2 -> 
                join "\n" $ for i.
                   (show i <> " " <>  (concat $ for j: (Fin 50).
                     select (((IToF (ordinal j)  + 1.0) / 50.0) < d2.i) "*" "" ) <> " " <> show d2.i)




' ## Random Variables

' ## Random Variables

' We consider random variables of three different forms.
  Latent, observed, sampled.

' 
  * Latent variables - Sum out explicitly
  * Observed variables - Assume observed
  * Sampled variables - Sample out in inference


' For latent and sampled variables `a=>Float` should be all 1.0s.
  This vector is used to compute marginals through autodiff.

def Var (a:Type) : Type =
    {latent : (a => Float) |
     observed : a |
     sample : (a=>Float & Key)}

def sample (var : Var a) (dist : ConDist a c) : ConDist a c =
    (AsProbsTable a) = dist
    AsProbsTable $ case var of
         {|latent=v|} -> 
              for i. v.i * a.i
         {|obs=v|} ->
              for i. select ((ordinal v) == (ordinal i)) a.i 0.0
         {|sample=(v, key)|} ->
              n = sum $ for i. a.i
              q = categorical (for i. log (a.i / n)) key
              for i. select ((ordinal q) == (ordinal i)) (v.i * a.i) 0.0


def cond_sample (var : DVar a) (dist : CDist a b) (prior : Probs b) : Probs (a & b) =
    (AsProbsTable p) = prior
    AsProbsTable $ case var of
         {|latent=v|} -> 
              for (i, j).
                  (AsProbsTable a) = dist.j
                  v.i * a.i * p.j
         {|obs=v|} ->
               for (i, j).
                  (AsProbsTable a) = dist.j
                  select ((ordinal v) == (ordinal i)) (a.i * p.j) 0.0
         {|sample=(v, key)|} ->
               v = for j.
                   (AsProbsTable a) = dist.j
                   n = sum $ for i. a.i * p.j
                   q = categorical (for i. log ((a.i * p.j) / n)) key
                   for i. select ((ordinal q) == (ordinal i)) (v.i * a.i * p.j) 0.0
               for (i,j). v.j.i



' Some useful distribution constructors

(true, false) = (1@(Fin 2), 0@(Fin 2))
Binary = Fin 2

def bernoulli (p: Float) : Probs Binary = AsProbsTable [1.0 - p, p]

def uniform : Probs m = AsProbsTable $ for i. 1.0 / (IToF (size m))

def delta (x:m) : Probs m =
    AsProbsTable $ yieldState zero $ \ aref.
               aref!x := 1.0


' ## Operations on probabilities




-- ' Apply the chain rule -  $p(a, b) = p(a | b) p(b)$

-- def chain (cond_dist : CDist a b) (prior : Probs b) : Probs (a & b) =
--     (AsProbsTable b) = prior
--     AsProbsTable $ for (j,i).
--                (AsProbsTable c) = cond_dist.i
--                b.i * c.j



    

' ## Random Variables

' We consider random variables of three different forms.
  Latent, observed, sampled.

' 
  * Latent variables - Sum out explicitly
  * Observed variables - Assume observed
  * Sampled variables - Sample out in inference


' For latent and sampled variables `a=>Float` should be all 1.0s.
  This vector is used to compute marginals through autodiff.

def DVar (a:Type) : Type = {latent : (a => Float) |
                            obs : a |
                            sample : (a=>Float & Key)}

def sample (var : DVar a) (dist : Probs a) : Probs a =
    (AsProbsTable a) = dist
    AsProbsTable $ case var of
         {|latent=v|} -> 
              for i. v.i * a.i
         {|obs=v|} ->
              for i. select ((ordinal v) == (ordinal i)) a.i 0.0
         {|sample=(v, key)|} ->
              n = sum $ for i. a.i
              q = categorical (for i. log (a.i / n)) key
              for i. select ((ordinal q) == (ordinal i)) (v.i * a.i) 0.0


def cond_sample (var : DVar a) (dist : CDist a b) (prior : Probs b) : Probs (a & b) =
    (AsProbsTable p) = prior
    AsProbsTable $ case var of
         {|latent=v|} -> 
              for (i, j).
                  (AsProbsTable a) = dist.j
                  v.i * a.i * p.j
         {|obs=v|} ->
               for (i, j).
                  (AsProbsTable a) = dist.j
                  select ((ordinal v) == (ordinal i)) (a.i * p.j) 0.0
         {|sample=(v, key)|} ->
               v = for j.
                   (AsProbsTable a) = dist.j
                   n = sum $ for i. a.i * p.j
                   q = categorical (for i. log ((a.i * p.j) / n)) key
                   for i. select ((ordinal q) == (ordinal i)) (v.i * a.i * p.j) 0.0
               for (i,j). v.j.i

' Helper functions because variants are annoying to write.

def var : DVar a = {|latent = for i. 1.0|}
def lat (x:a=>Float) : DVar a = {|latent = x|}



' Marginalization is done by differentiating the log-evidence.

def marginalize [Mul a] (f : a -> Float) : a =
    (grad (\ x. log $ f x)) one


' `??` is a helper function for computing the probability of an event. 

def (??) (dist: Probs a) (x: a=>Bool): Float =
    (AsProbsTable d) = dist
    sum $ for i. d.i * select x.i 1.0 0.0


' ## Examples 1: Sprinkers

' This is the famous sprinkler problem used to demonstrate Bayes' Nets.
  This example implements https://en.wikipedia.org/wiki/Bayesian_network


Rain = Binary
Sprinkler = Binary
Wet = Binary
               


' Case where all the variables are True. $P(G=T, S=T, R=T)$

show $ wet {|obs=true|} {|obs=true|} {|obs=true|}

(rain, sprinkle) = marginalize (\ (v2, v3). wet (lat v2) (lat v3) (var) ?? [False, True])


' Marginal probability of rain given wet grass $P(R=T | G=T)$

show $ AsProbsTable rain


' This shows that we can sample from the distribution using the same code.

[k1, k2] = splitKey $ newKey 0

wet {|sample=(one, k1)|} {|sample=(one,k2)|} var 



' ## Example 2: Dice

' Dice values as represented by Fin

Faces = Fin 6
def diceval (x: Faces) : Int = (ordinal x) + 1


def dice ((dice_vars1, dice_vars2, sum_var): (DVar Faces & DVar Faces & DVar (Fin 13))) :
      Probs (Fin 13) =
    dice1  = sample dice_vars1 $ uniform
    dice2  = sample dice_vars2 $ uniform

    -- Sum of dice rolls as a CPT table
    sumdice = for (i, j). delta (((diceval i) + (diceval j))@_)

    -- Dice are independent
    sample sum_var $ sumdice <| (dice1 ,, dice2)

' Sum of two dice

show $ dice (var, var, var)

-- Sum of two dice with one fixed.

dice (var, {|obs= (3@_)|}, var)


-- Dice values conditioned on summing to 3
marginalize (\ (d1, d2). dice (lat d1, lat d2, {|obs=(3@_)|} ) ?? for i. True)

' Sample through

dice ({|sample= (one, k1)|}, {|sample=(one, k2)|}, var)



' ## Example 3: HMM

' Let's us easily generate arbitrary probability distributions

instance [Arbitrary a] Arbitrary (Probs a)
  arb = \k .
      x = arb k
      n = sum for i. abs x.i 
      AsProbsTable for i. (abs x.i) / n


' Very concise implementation of an HMM

Z = Fin 5
X = Fin 10


def hmm ((hidden_vars, init_var): (m => DVar Z & DVar Z)) (x: m => X)
        (transition : CDist Z Z) (emission: CDist X Z)
    : Probs Z =

    -- Sample an initial state
    yieldState (sample init_var uniform) \zref .
       for i.
           -- Sample next state
           z' = sample hidden_vars.i $ transition <| (get zref)

           -- Factor in evidence
           zref := obs x.i z' emission    

' Compute an HMM over a simple sequence

show $ hmm (for i:(Fin 5). var, var) (for i. (0@_)) (arb k1) (arb k2)

' Do marginalization using a autograd (Currently doesn't work yet)

-- marginalize (\ (x0, x1, x2, x3, x4). hmm [Latent x0, Latent x1, Latent x2, Latent x3, Latent x4]
--            (for i:(Fin 5). (0@_)))

