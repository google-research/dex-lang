' # Neural Network

' ## Declare some helpers

' ### Rand helpers
like `ixkey` but takes 2 index to use as keys

ixkey2:: A n::Ix m::Ix. Key -> n -> m -> Key
ixkey2 x i = ixkey $ ixkey x i

:p k = newKey 0
   for i::2 j::3. sq $ randn (ixkey2 k i j)
> [[1.773118, 0.50770324, 7.503997e-2], [1.4671916, 5.4218978e-2, 0.5504309]]



' ## Matrix Math

' Vector-Vector dot product

vdot :: n=>Real -> n=>Real -> Real
vdot x y = sum for i. x.i * y.i

' Matrix Matrix Product

mmp :: l=>m=>Real -> m=>n=>Real -> l=>n=>Real
mmp m1 m2 = for i k. sum for j. m1.i.j * m2.j.k

' Matrix Vector product

mvp :: n=>m=>Real -> m=>Real -> n=>Real
mvp m v = for i. vdot m.i v


mvp ..
    [[10.0, 20.0], [30.0, 40.0], [50.0, 60.0]] ..
    [2.0, 4.0]
> [100.0, 220.0, 340.0]




' ---
## Neural Network functions
Note: all out activation and loss functions apply to vectors,
where that vector represents a single observation.
e.g. binaryCrossEntropy takes in `1=>Real`. `sigmoid` takes in `d=>Real`

' ### Activation Functions 


' scalar sigmoid

sigmoid1:: Real->Real
sigmoid1 x = 1.0/(1.0 + exp (neg x))

:p sigmoid1 0.0
> 0.5

' vector broadcast sigmoid

sigmoid:: (d=>Real)->(d=>Real)
sigmoid = map sigmoid1

:p sigmoid [-1000.0, 0.0, 1000.0]
> [0.0, 0.5, 1.0]

' softmax, with overflow resistance

softmax:: d=>Real->d=>Real
softmax xs = ..
    numer = for i. exp(xs.i - maximum(xs))
    for i. numer.i / sum(numer)


softmax [10000.0, 0.0]
> [1.0, 0.0]

softmax [2.0, 2.0, 2.0, 2.0]
> [0.25, 0.25, 0.25, 0.25]

' ### Loss functions

binaryCrossEntropy:: (1=>Real)->(1=>Real)->Real
binaryCrossEntropy target output = ..
    tar = target.(0@1)
    out = output.(0@1)
    eps = 0.000000000000001
    comp x = 1.0 - x
    fcrossent f = f(tar) * log(out + eps)
    (fcrossent id) + (fcrossent comp)

binaryCrossEntropy [1.0] [0.99]

binaryCrossEntropy [1.0] [0.01]

binaryCrossEntropy [0.01] [1.0]


' ### Layers
The type of a Affine layer is: `(d=>n=>Real, d=>Real)`.
That is the weight matrix and the bias vector.

initAffine:: A n::Ix d::Ix. Key->(d=>n=>Real, d=>Real)
initAffine key = ..
    (weightKey, biasKey) = splitKey key
    weights = for x::d. for y::n.
        0.1 * (randn $ ixkey2 weightKey x y)
    biases = for y::d.
        0.1 * (randn $ ixkey biasKey y)
    (weights, biases)

initAffine @3 @2 (newKey 0)
> ( [ [-1.6417312e-2, -0.11909912]
> , [-1.777558e-2, 8.824792e-2]
> , [0.12012412, 0.12777644] ]
> , [0.12112768, 2.3284968e-2] )

applyAffine:: (d=>n=>Real, d=>Real) -> (n=>Real) -> (d=>Real)
applyAffine (w, b) z = for k::d.
    (mvp w z).k + b.k


' try it out

layer1 = initAffine @3 @2 (newKey 1)
input1 = [1.0, 2.0, 3.0]
sigmoid $ applyAffine layer1 input1
> [2.863232e-2, 0.22663473]

' ## XOR Task

xor:: Bool -> Bool -> Bool
xor a b = (a && (not b)) || ((not a) && b)

xor True True
> False

xor False False
> False

xor True False
> True

xor False True
> True

trueXorClass:: (2=>Real)->Bool
trueXorClass x = xor (x.(0@2) > 0.5) (x.(1@2) > 0.5)

trueXorClass [0.3, 0.3]
> False

trueXorClass [0.3, 0.7]
> True

trueXorClass [0.7, 0.3]
> True

trueXorClass [0.7, 0.7]
> False

xorTrainInputs = for i::10. for j::2.
    trainKey = newKey 42
    rand (ixkey2 trainKey i j)

xorTrainTargetClasses = map trueXorClass xorTrainInputs
xorTrainTargets = for i. for j::1.
    b2r xorTrainTargetClasses.i


xorTrainInputs

xorTrainTargetClasses

xorTrainTargets

(filter trueXorClass xorTrainInputs).(asidx 1)

' ## Xor Net
This is a two layer net.
AFAICT no data structure in Dex right now can hold a arbitrary collection
of neural network layers/parameters, as it would be a heterogeneous vector, or a tuple of non-fixed length
As such need different functions for each depth of network

xorNetParams = ( ..
    initAffine @2 @6 (newKey 10), ..
    initAffine @6 @1 (newKey 20) ..
)

:t xorNetParams

apply2Net:: ((hz=>(iz=>Real), hz=>Real), (oz=>(hz=>Real), oz=>Real)) -> iz=>Real -> oz=>Real
apply2Net (layer1, layer2) input = 
    sigmoid $ applyAffine layer2 $ sigmoid $ applyAffine layer1 input

apply2Net xorNetParams [0.9, 0.1]

(binaryCrossEntropy [1.0]) $ apply2Net xorNetParams [0.9, 0.1]

eval2NetLoss:: (oz=>Real->oz=>Real->Real) -> oz=>Real -> iz=>Real -> ((hz=>(iz=>Real), hz=>Real), (oz=>(hz=>Real), oz=>Real)) -> Real
eval2NetLoss loss target input netParams = ..
    loss target $ apply2Net netParams input

eval2NetLoss binaryCrossEntropy [1.0] [0.9, 0.1] xorNetParams 

xorLoss:: ((hz=>(2=>Real), hz=>Real), (1=>(hz=>Real), 1=>Real)) -> Real
xorLoss = eval2NetLoss binaryCrossEntropy [1.0] [0.9, 0.1]


grad xorLoss xorNetParams
