<!DOCTYPE HTML>
<html><head><style type="text/css">/* Copyright 2019 Google LLC                              */
/*                                                        */
/* Use of this source code is governed by a BSD-style     */
/* license that can be found in the LICENSE file or at    */
/* https://developers.google.com/open-source/licenses/bsd */

body {
  margin: 1em auto 1em auto;
  padding: 1em 1em 1em 1em;
  max-width: 50em;
  font-family: Helvetica, sans-serif;
  font-size: 100%;
  color: #333;
  padding-bottom: 500px;
}

.cell {
}

.code-block, .err-block, .result-block {
  padding: 0em 0em 0em 2em;
  display: block;
  font-family: monospace;
  white-space: pre;
}

code {
  background-color: #F0F0F0;
}

.result-block {
  border-left: 3px solid  #87CEFA;
}

.prose-block {
  line-height: 140%;
}

.err-block {
  font-weight: bold;
  color: #B22222;
  border-left: 3px solid #B22222;
}

.plot {
  padding: 5em;
}

.plot-img {
  width: 80%;
}

.comment {
  color: #808080;
}

.keyword {
  color: #0000DD;
}

.command {
  color: #A80000;
}

.symbol {
  color: #E07000;
}

.type-name {
  color: #A80000;
}

.iso-sugar {
  color: #25BBA7;
}
</style><meta charset="UTF-8"></head><body><div id="main-output"><div class="cell"><div class="code-block">
</div></div><div class="cell"><div class="prose-block"><h2>Stochastic Gradient Descent with Momentum</h2>
</div></div><div class="cell"><div class="code-block"><span class="keyword">def</span> sgd_step [<span class="type-name">VSpace</span> a] (step_size<span class="symbol">:</span> <span class="type-name">Float</span>) (decay<span class="symbol">:</span> <span class="type-name">Float</span>) (gradfunc<span class="symbol">:</span> a <span class="symbol">-&gt;</span> <span class="type-name">Int</span> <span class="symbol">-&gt;</span> a) (x<span class="symbol">:</span> a) (m<span class="symbol">:</span> a) (iter<span class="symbol">:</span><span class="type-name">Int</span>) <span class="symbol">:</span> (a <span class="symbol">&amp;</span> a) <span class="symbol">=</span>
  g <span class="symbol">=</span> gradfunc x iter
  new_m <span class="symbol">=</span> decay <span class="symbol">.*</span> m <span class="symbol">+</span> g
  new_x <span class="symbol">=</span> x <span class="symbol">-</span> step_size <span class="symbol">.*</span> new_m
  (new_x<span class="symbol">,</span> new_m)
</div></div><div class="cell"><div class="code-block">
</div></div><div class="cell"><div class="code-block"><span class="comment">-- In-place optimization loop.
</span></div></div><div class="cell"><div class="code-block"><span class="keyword">def</span> sgd [<span class="type-name">VSpace</span> a] (step_size<span class="symbol">:</span><span class="type-name">Float</span>) (decay<span class="symbol">:</span><span class="type-name">Float</span>) (num_steps<span class="symbol">:</span><span class="type-name">Int</span>) (gradient<span class="symbol">:</span> a <span class="symbol">-&gt;</span> <span class="type-name">Int</span> <span class="symbol">-&gt;</span> a) (x0<span class="symbol">:</span> a) <span class="symbol">:</span> a <span class="symbol">=</span>
  m0 <span class="symbol">=</span> zero
  (x_final<span class="symbol">,</span> m_final) <span class="symbol">=</span> yieldState (x0<span class="symbol">,</span> m0) <span class="symbol">\</span>state<span class="symbol">.</span>
    <span class="keyword">for</span> i<span class="symbol">:</span>(<span class="type-name">Fin</span> num_steps)<span class="symbol">.</span>
      (x<span class="symbol">,</span> m) <span class="symbol">=</span> get state
      state <span class="symbol">:=</span> sgd_step step_size decay gradient x m (ordinal i)
  x_final
</div></div><div class="cell"><div class="code-block">

</div></div><div class="cell"><div class="prose-block"><h3>Example quadratic optimization problem</h3>
</div></div><div class="cell"><div class="code-block"><span class="type-name">D</span> <span class="symbol">=</span> <span class="type-name">Fin</span> 4
</div></div><div class="cell"><div class="code-block">optimum <span class="symbol">=</span> <span class="keyword">for</span> i<span class="symbol">:</span><span class="type-name">D</span><span class="symbol">.</span> 1<span class="symbol">.</span>1
</div></div><div class="cell"><div class="code-block"><span class="comment">-- Hand-coded gradient until autodiff works
</span></div></div><div class="cell"><div class="code-block"><span class="keyword">def</span> gradfunc (x<span class="symbol">:</span><span class="type-name">D</span><span class="symbol">=&gt;</span><span class="type-name">Float</span>) (iter<span class="symbol">:</span><span class="type-name">Int</span>) <span class="symbol">:</span> <span class="type-name">D</span><span class="symbol">=&gt;</span><span class="type-name">Float</span> <span class="symbol">=</span> x <span class="symbol">-</span> optimum
</div></div><div class="cell"><div class="code-block">
</div></div><div class="cell"><div class="prose-block"><h3>Run optimizer</h3>
</div></div><div class="cell"><div class="code-block">x_init <span class="symbol">=</span> <span class="keyword">for</span> i<span class="symbol">:</span><span class="type-name">D</span><span class="symbol">.</span> 0<span class="symbol">.</span>0
</div></div><div class="cell"><div class="code-block">stepsize <span class="symbol">=</span> 0<span class="symbol">.</span>01
</div></div><div class="cell"><div class="code-block">decay <span class="symbol">=</span> 0<span class="symbol">.</span>9
</div></div><div class="cell"><div class="code-block">num_iters <span class="symbol">=</span> 1000
</div></div><div class="cell"><div class="code-block"><span class="command">:p</span> sgd stepsize decay num_iters gradfunc x_init
</div><div class="result-block">[1.1, 1.1, 1.1, 1.1]</div></div><div class="cell"><div class="code-block">
</div></div><div class="cell"><div class="code-block"><span class="command">:p</span> optimum
</div><div class="result-block">[1.1, 1.1, 1.1, 1.1]</div></div></div></body></html>