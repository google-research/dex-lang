'# Kernel Regression

import linalg
import stats
import plot

struct ConjGradState(a|VSpace) =
  x : a
  r : a
  p : a

-- Conjugate gradients solver
def solve'(mat:m=>m=>Float, b:m=>Float) -> m=>Float given (m|Ix) =
  x0 = zero
  ax = mat **. x0
  r0 = b - ax
  init = ConjGradState(zero, r0, r0)
  final = fold init \_:m state.
    x = state.x
    r = state.r
    p = state.p
    ap = mat **. p
    alpha = vdot r r / vdot p ap
    x' = x + alpha .* p
    r' = r - alpha .* ap
    beta = vdot r' r' / (vdot r r + 0.000001)
    p' = r' + beta .* p
    ConjGradState(x', r', p')
  final.x

def chol_solve(l:LowerTriMat m Float, b:m=>Float) -> m=>Float given (m|Ix) =
    b' = forward_substitute l b
    u = transpose_lower_to_upper l
    backward_substitute u b'

' # Kernel ridge regression

' To learn a function $f_{true}: \mathcal{X} \to \mathbb R$
from data $(x_1, y_1),\dots,(x_N, y_N)\in \mathcal{X}\times\mathbb R$,\
in kernel ridge regression the hypothesis takes the form
$f(x)=\sum_{i=1}^N \alpha_i k(x_i, x)$,\
where $k:\mathcal X \times \mathcal X \to \mathbb R$ is a positive semidefinite kernel function.\
The optimal coefficients are found by solving a linear system $\alpha=G^{-1}y$,\
where $G_{ij}:=k(x_i, x_j)+\delta_{ij}\lambda$, $\lambda>0$ and $y = (y_1,\dots,y_N)^\top\in\mathbb R^N$

-- Synthetic data
Nx = Fin 20
noise = 0.1
[k1, k2] = split_key (new_key 0)

def trueFun(x:Float) -> Float =
  x + sin (20.0 * x)

xs : Nx=>Float = for i. rand (ixkey k1 i)
ys : Nx=>Float = for i. trueFun xs[i] + noise * randn (ixkey k2 i)

-- Kernel ridge regression
def regress(kernel: (a, a) -> Float, xs: Nx=>a, ys: Nx=>Float) -> (a) -> Float given (a) =
    gram = for i j. kernel xs[i] xs[j] + select (i==j) 0.0001 0.0
    alpha = solve' gram ys
    \x. sum for i. alpha[i] * kernel xs[i] x

def rbf(lengthscale:Float, x:Float, y:Float) -> Float =
  exp (-0.5 * sq ((x - y) / lengthscale))

predict = regress (\x y. rbf 0.2 x y) xs ys

-- Evaluation
Nxtest = Fin 1000
xtest : Nxtest=>Float = for i. rand (ixkey k1 i)
preds = map predict xtest

-- True function.
:html show_plot $ xy_plot xtest (map trueFun xtest)
> <html output>

-- Observed values.
:html show_plot $ xy_plot xs ys
> <html output>

-- Ridge regression prediction.
:html show_plot $ xy_plot xtest preds
> <html output>

' # Gaussian process regression

' GP regression (kriging) works in a similar way. Compared with kernel ridge regression, GP regression assumes Gaussian distributed prior. This, combined
with the Bayes rule, gives the variance of the prediction.

' In this implementation, the conjugate gradient solver is replaced with the
cholesky solver from `lib/linalg.dx` for efficiency.

def gp(
  kernel: (a, a) -> Float,
  xs: n=>a,
  mean_fn: (a) -> Float,
  noise_var: Float
) -> MultivariateNormal n given (n|Ix, a) =
    gram = for i j. kernel xs[i] xs[j]
    loc = for i. mean_fn xs[i]
    chol_cov = chol (gram + eye *. noise_var)
    MultivariateNormal loc chol_cov

def gp_regress(
  kernel: (a, a) -> Float,
  xs: n=>a,
  ys: n=>Float,
  mean_fn: (a) -> Float,
  noise_var: Float
) -> ((m=>a) -> MultivariateNormal m) given (n|Ix, m|Ix, a) =
    prior_gp = gp kernel xs mean_fn noise_var
    gram_obs_inv_y = chol_solve prior_gp.chol_cov (ys - prior_gp.loc)
    predictive_gp_fn = \xs_pred:m=>a.
      gram_pred_obs = for i j. kernel xs_pred[i] xs[j]
      loc = gram_pred_obs **. gram_obs_inv_y + (for i. mean_fn xs_pred[i])
      gram_pred = (for i j. kernel xs_pred[i] xs_pred[j]) + eye *. noise_var
      gram_obs_inv_gram_pred_obs = for i. chol_solve prior_gp.chol_cov gram_pred_obs[i]
      schur = gram_pred - gram_obs_inv_gram_pred_obs ** (transpose gram_pred_obs)
      MultivariateNormal loc (chol schur)
    predictive_gp_fn

def mean_fn(x:Float) -> Float = 0. * x
gp_predict_fn : (Nxtest=>Float) -> MultivariateNormal Nxtest = gp_regress (\x y. rbf 0.2 x y) xs ys mean_fn 0.0001 
gp_predict_dist = gp_predict_fn xtest
var_pred = for i. vdot gp_predict_dist.chol_cov[i] gp_predict_dist.chol_cov[i]

-- GP posterior predictive mean, colored by variance.
:html show_plot $ xyc_plot xtest gp_predict_dist.loc (map sqrt var_pred)
> <html output>

-- Posterior predictive variance.
:html show_plot $ xy_plot xtest var_pred
> <html output>
