' # Neural Networks
This example builds a mini library of NN layers
for deep learning.  The goal is to demonstrate how to specify these
generic layers within the type system as well to experiment with
usable designs.

' Before running this example, get Fashion MNist binary files using the commands:

' `wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz; gunzip t10k-images-idx3-ubyte.gz`
' `wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz; gunzip t10k-labels-idx1-ubyte.gz`


import plot

' ## NN Prelude
This section provides basics needed for NNs.


def relu (input : Float) : Float =
  select (input > 0.0) input 0.0

instance [Add a, Add b] Add (a & b)
  add = \(a, b) (c, d). ( (a + c), (b + d))
  sub = \(a, b) (c, d). ( (a - c), (b - d))
  zero = (zero, zero)

instance [VSpace a, VSpace b] VSpace (a & b)
  scaleVec = \ s (a, b) . (scaleVec s a, scaleVec s b)


' ## Layers
A neural network layer pairs an initilization function
for creating parameters with a forward function that
utilizes these parameters for transforming its input.


data Layer inp:Type out:Type params:Type =
  AsLayer {forward:(params -> inp -> out) & init:(Key -> params)}


' These helper functions apply a layer.

def forward (l:Layer i o p) (p : p) (x : i): o =
  (AsLayer l') = l
  (getAt #forward l') p x

def init (l:Layer i o p) (k:Key)  : p  =
  (AsLayer l') = l
  (getAt #init l') k


' ## Example Layers 

' ### Dense layer
A dense layer applies an affine transformation to its input.
The parameters are a matrix weight and a bias. 

def DenseParams (in:Type) (out:Type) : Type =
   ((in=>out=>Float) & (out=>Float))


' Constructor for a Dense layer

def dense (in:Type) (out:Type) :
   Layer (in=>Float) (out=>Float) (DenseParams in out) =
  AsLayer {
    forward = (\ (weight, bias) x .
               for o. (bias.o + sum for i. weight.i.o * x.i)),
    init = arb
   }
  

' ### CNN layer
CNN Layers are a bit more complex. We first define a
2D Convolution in its generic form. (This function is
still quite slow in Dex)

def conv2d (x:inc=>(Fin h)=>(Fin w)=>Float)
           (kernel:outc=>inc=>(Fin kh)=>(Fin kw)=>Float) :
     outc=>(Fin h)=>(Fin w)=>Float =
     for o i j.
         (i', j') = (ordinal i, ordinal j)
         case (i' + kh) < h && (j' + kw) < w of
          True ->
              sum for (ki, kj, inp).
                  (di, dj) = (fromOrdinal (Fin h) (i' + (ordinal ki)),
                              fromOrdinal (Fin w) (j' + (ordinal kj)))
                  x.inp.di.dj * kernel.o.inp.ki.kj
          False -> zero


' Given in and out channels and kernel size, the CNN parameters
are a table of kernels and a bias vector


def CNNParams (inc:Type) (outc:Type) (kh:Int) (kw:Int) : Type =
  ((outc=>inc=>Fin kh=>Fin kw=>Float) &
   (outc=>Float))


' A cnn layer is should work with any input height and width.
  Its implmentation just call the conv2d function.

def cnn (h:Int) ?-> (w:Int) ?-> (inc:Type) (outc:Type) (kh:Int) (kw:Int) :
    Layer (inc=>(Fin h)=>(Fin w)=>Float)
          (outc=>(Fin h)=>(Fin w)=>Float)
          (CNNParams inc outc kw kh) =
  AsLayer {
    forward = (\ (weight, bias) x. for o i j . (conv2d x weight).o.i.j + bias.o),
    init = arb
  }

' ### Pooling
Pooling functions do not need to be defined as layers since they
has no parameters. They simply work as standard Dex functions.

def split (x: m=>v) : n=>o=>v =
    for i j. x.((ordinal (i,j))@m)
            
def imtile (x: a=>b=>v) : n=>o=>p=>q=>v =
    for kw kh w h. (split (split x).w.kw).h.kh

def meanpool (kh: Type) (kw: Type) (x : m=>n=>Float) : (h=>w=>Float) =
    out : (kh => kw => h => w => Float) = imtile x
    mean for (i,j). out.i.j

' ## Simple point classifier
To test out our library, we will create some data representing an
XOR classification problem on 2d points. This data requires a simple
NN in order to be classified correctly. 


[k1, k2] = splitKey $ newKey 1
x1 : Fin 100 => Float = arb k1
x2 : Fin 100 => Float = arb k2
y = for i. case ((x1.i > 0.0) && (x2.i > 0.0)) || ((x1.i < 0.0) && (x2.i < 0.0)) of
  True -> 1
  False -> 0
xs = for i. [x1.i, x2.i]


:html showPlot $ xycPlot x1 x2 $ for i. IToF y.i
> <html output>

' The model is a simple network that first applies an affine
to the hidden dim size, then a relu, then an affine to the output
class size.

simple = \h1.
  ldense1 = dense (Fin 2) h1
  ldense2 = dense h1 (Fin 2)
  AsLayer {
    forward = (\ (pdense1, pdense2) x.
         x1' = forward ldense1 pdense1 x
         x1 = for i. relu x1'.i
         logsoftmax $ forward ldense2 pdense2 x1),
    init = (\key.
         [k1, k2] = splitKey key
         (init ldense1 k1, init ldense2 k2))
  }

:t simple
> ((h1:Type)
>  -> Layer
>       ((Fin 2) => Float32)
>       ((Fin 2) => Float32)
>       ( (((Fin 2) => h1 => Float32) & (h1 => Float32))
>       & ((h1 => (Fin 2) => Float32) & ((Fin 2) => Float32))))

' In order to train with this model we need a training class.
Here we take in the model, data, epoches, and minibatch size.
Training is done with simple SGD with a fixed learning rate.

def trainClass [VSpace p] (model: Layer a (b=>Float) p)
                           (x: batch=>a)
                           (y: batch=>b)
                           (epochs : Type)
                           (minibatch : Type)
                           (minibatches : Type) :
    (epochs => p & epochs => Float ) =
  xs : minibatches => minibatch => a = split x
  ys : minibatches => minibatch => b = split y
  unzip $ withState (init model $ newKey 0) $ \params .
     for _ : epochs.
       loss = sum $ for b : minibatches. 
              (loss, gradfn) =  vjp (\ params.
                            -sum for j.
                                       result = forward model params xs.b.j
                                       result.(ys.b.j)) (get params)
              gparams = gradfn 1.0
              params := (get params) - scaleVec (0.05 / (IToF (size minibatch))) gparams
              loss
       (get params, loss)

' Now we can train our model. Here we create one of hidden size 10 and
run it for 500 epochs. 

simple_model = simple (Fin 10)
(all_params,losses) = trainClass simple_model xs (for i. (y.i @ (Fin 2))) (Fin 500) (Fin 100) (Fin 1)


' This code plots the resulting decision space and the input points. 

span = linspace (Fin 10) (-1.0) (1.0)
tests = for h : (Fin 50). for i . for j.
        r = forward simple_model all_params.((ordinal h * 10)@_) [span.i, span.j]
        [exp r.(1@_), exp r.(0@_), 0.0]
        

:html imseqshow tests
> <html output>

' ## LeNet for image classification
Here we construct a more complex neural network. Specifically
a version of the LeNet CNN model (https://en.wikipedia.org/wiki/LeNet).
This model works on images of size 28x28 with 1 channel, and classifies
into 10 classes. 


H = 28
W = 28
Image = Fin 1 => Fin H => Fin W => Float 
Class = Fin 10


' The model itself requires three hidden dimension sizes.
It runs two 2 CNNs, mean pooling, and then a final two affine
layers.

lenet = \h1 h2 h3 .
  ncnn1 = cnn (Fin 1) h1 3 3
  ncnn2 = cnn h1 h2 3 3
  Pooled = (h2 & Fin 7 & Fin 7)
  ndense1 = dense Pooled h3
  ndense2 = dense h3 Class
  AsLayer {
    forward = (\ (cnn1, cnn2, dense1, dense2) inp.
         x:Image = inp
         x1' = forward ncnn1 cnn1 x
         x1 = for i j k. relu x1'.i.j.k
         x2' = forward ncnn2 cnn2 x1
         x2 = for i j k. relu x2'.i.j.k
         x3 : (h2 => Fin 7 => Fin 7 => Float) = for c. meanpool (Fin 4) (Fin 4) x2.c
         x4' = forward ndense1 dense1 for (i,j,k). x3.i.j.k     
         x4 = for i. relu x4'.i
         logsoftmax $ forward ndense2 dense2 x4),
    init = (\key.
         [k1, k2, k3, k4] = splitKey key
         (init ncnn1 k1, init ncnn2 k2,
         init ndense1 k3, init ndense2 k4))
  }

:t lenet
> ((h1:Type)
>  -> (h2:Type)
>  -> (h3:Type)
>  -> Layer
>       ((Fin 1) => (Fin 28) => (Fin 28) => Float32)
>       ((Fin 10) => Float32)
>       ( ((h1 => (Fin 1) => (Fin 3) => (Fin 3) => Float32) & (h1 => Float32))
>       & ( ((h2 => h1 => (Fin 3) => (Fin 3) => Float32) & (h2 => Float32))
>         & ( (((h2 & (Fin 7 & Fin 7)) => h3 => Float32) & (h3 => Float32))
>           & ((h3 => (Fin 10) => Float32) & ((Fin 10) => Float32))))))


' ### Fashion MNist
To test the model out, we load data from the Fashion MNist dataset.

Batch = Fin 5000
Full = Fin ((size Batch) * H * W)

def pixel (x:Char) : Float32 =
     r = W8ToI x
     IToF case r < 0 of
             True -> (abs r) + 128
             False -> r

def getIm : Batch => Image =
    -- File is unsigned bytes offset with 16 starting bytes
    (AsList _ im) = unsafeIO do readFile "examples/t10k-images-idx3-ubyte"
    raw = unsafeCastTable Full (for i:Full. im.((ordinal i + 16) @ _))
    for b: Batch c i j.
        pixel raw.((ordinal (b, i, j)) @ Full)

def getLabel : Batch => Class =
    -- File is unsigned bytes offset with 8 starting bytes
    (AsList _ lab) = unsafeIO do readFile "examples/t10k-labels-idx1-ubyte"
    r = unsafeCastTable Batch (for i:Batch. lab.((ordinal i + 8) @ _))
    for i. (W8ToI r.i @ Class)

' ### Training loop
This code runs the model over the given
data. Currently this training process is a bit slow do to the
convolution implemementation but we expect it to get faster.

ims = getIm
labels = getLabel

small_ims = for i: (Fin 10). ims.((ordinal i)@_)
small_labels = for i: (Fin 10). labels.((ordinal i)@_)

:p small_labels
> [ (9@Fin 10)
> , (2@Fin 10)
> , (1@Fin 10)
> , (1@Fin 10)
> , (6@Fin 10)
> , (1@Fin 10)
> , (4@Fin 10)
> , (6@Fin 10)
> , (5@Fin 10)
> , (7@Fin 10) ]

Epochs = (Fin 1)
Minibatches = (Fin 1)
Minibatch = (Fin 10)

:t ims.(2@_)
> ((Fin 1) => (Fin 28) => (Fin 28) => Float32)

model = lenet (Fin 1) (Fin 1) (Fin 10) 
init_param = (init model  $ newKey 0)
:p forward model init_param (ims.(2@Batch))
> [ -2139.577
> , -708.1797
> , 0.
> , -1464.5243
> , -2020.7612
> , -356.19592
> , -816.3495
> , -1105.3662
> , -1729.139
> , -1461.0334 ]

' Sanity check

:t (grad ((\x param. sum (forward model param x)) (ims.(2@_)))) init_param
> ( (((Fin 1) => (Fin 1) => (Fin 3) => (Fin 3) => Float32) & ((Fin 1) => Float32))
> & ( ( ((Fin 1) => (Fin 1) => (Fin 3) => (Fin 3) => Float32)
>     & ((Fin 1) => Float32))
>   & ( ( ((Fin 1 & (Fin 7 & Fin 7)) => (Fin 10) => Float32)
>       & ((Fin 10) => Float32))
>     & (((Fin 10) => (Fin 10) => Float32) & ((Fin 10) => Float32)))))

(all_params', losses') = trainClass model small_ims small_labels Epochs Minibatch Minibatches

:p losses'
> [15243.2]
