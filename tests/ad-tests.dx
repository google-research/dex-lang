
-- TODO: use prelude sum instead once we can differentiate state effect
def sum' {n} (xs:n=>Float) : Float = yieldAccum (AddMonoid Float) \ref. for i. ref += xs.i

:p
   f : Float -> Float = \x. x
   jvp f 3.0 2.0
> 2.

:p
   f = \x. x * x
   jvp f 3.0 1.5
> 9.

:p
   f = \x. x + x
   jvp f 3.0 2.0
> 4.

:p
   f = \x. x * x * x
   jvp f 2.0 1.5
> 18.

:p
   f : Float --o Float = \x. x
   transpose_linear f 2.0
> 2.

:p
   f : Float --o Float = \x. x + x
   transpose_linear f 1.0
> 2.

:p
   f : Float --o Float = \x. x + (x + x) * 2.0
   transpose_linear f 1.0
> 5.

:p
   f : Float --o Float = \x. x * 2.0
   transpose_linear f 1.0
> 2.

:p
   f : Float --o Float = \x. 2.0 * x
   transpose_linear f 1.0
> 2.

:p grad (\x. x * x) 1.0
> 2.

:p deriv (\x. 3.0 / x) 2.0
> -0.75

:p deriv (\x. x / 2.0) 3.0
> 0.5

:p
  f : n:Type ?-> n=>Float -> n=>Float =
   \xs. for i. xs.i * xs.i

  jvp f [1.,2.] [3.,4.]
> [6., 16.]

:p jvp transpose [[1.,2.], [3.,4.]] [[10.,20.], [30.,40.]]
> [[10., 30.], [20., 40.]]

:p jvp sum' [1., 2.] [10.0, 20.0]
> 30.

f : Float -> Float = \x. yieldAccum (AddMonoid Float) \ref. ref += x
:p jvp f 1.0 1.0
> 1.

:p
   f = \x. x * x * x
   jvp (\x. jvp f x 1.0) 2.0 1.0
> 12.

:p
   f = \x. 4.0 * x * x * x
   deriv (deriv (deriv f)) 1.234
> 24.

:p
   f : Float --o (Float & Float) = \x. (x, 2.0 * x)
   transpose_linear f (1.0, 3.0)
> 7.

:p
   f : (Float & Float) --o Float = \(x,y). x + 2.0 * y
   transpose_linear f 1.0
> (1., 2.)

:p deriv cos 0.0
> 0.

:p deriv sin 0.0
> 1.

:p (sin 1.0, deriv (deriv sin) 1.0)
> (0.841471, -0.841471)

:p (cos 1.0, deriv (deriv (deriv sin)) 1.0)
> (0.540302, -0.540302)

:p check_deriv sin 1.0
> True

:p check_deriv cos 1.0
> True

:p check_deriv exp 2.0
> True

:p check_deriv log 2.0
> True

:p check_deriv (\x. exp (sin (cos x))) 2.0
> True

:p check_deriv (deriv sin) 1.0
> True

:p check_deriv (deriv cos) 1.0
> True

:p check_deriv sqrt 4.0
> True

-- badDerivFun : Float -> Float
-- badDerivFun x = x

-- badDerivFun#lin : Float -> (Float, Float --o Float)
-- badDerivFun#lin x = (x, llam t. 2. * t)

-- :p checkDeriv badDerivFun 1.0
-- > False

-- Perturbation confusion test suggested by Barak Pearlmutter
-- https://github.com/HIPS/autograd/issues/4
:p deriv (\x. x * deriv (\y. x * y) 2.0) 1.0
> 2.

tripleit : Float --o Float = \x. x + x + x

:p tripleit 1.0
> 3.

:p transpose_linear tripleit 1.0
> 3.

:p transpose_linear (transpose_linear tripleit) 1.0
> 3.

:p
  f : n:Type ?-> Ix n ?=> Float --o n=>Float = \x. for i. x

  transpose_linear f [1.0, 2.0]
> 3.

:p
  f : n:Type ?-> n=>Float --o n=>Float = \x. for i. x.i * 2.0

  transpose_linear f [1.0, 2.0]
> [2., 4.]

myOtherSquare : Float -> Float =
  \x. yieldAccum (AddMonoid Float) \w. w += x * x

:p check_deriv myOtherSquare 3.0
> True

:p
  f : Float -> Float =
    \x. fst (x * x, 2 + 1)

  jvp f 1.0 3.0
> 6.

:p
  f : Float -> Float = \x.
    x * i_to_f (1 + 1)

  jvp f 1.0 2.0
> 4.

:p
  f : (Fin 2)=>Float -> Float =
    \xs. xs.(0 @ Fin 2) * xs.(1 @ Fin 2)

  jvp f [1., 2.] [3.0, 4.0]
> 10.

:p
  f : (Float & Float) -> Float =
    \(x,y). x * y

  jvp f (1., 2.) (3.0, 4.0)
> 10.

:p
  f : n:Type ?-> n=>Float -> n=>Float =
    \xs. for i. xs.i * xs.i

  jvp f [1.,2.] [3.,4.]
> [6., 16.]

:p jvp sum' [1., 2.] [3.0, 4.0]
> 7.

:p grad sum' [1.,2.]
> [1., 1.]


vec = [1.]

:p jvp (\x. vec) [1.] [1.]
> [0.]

:p grad (\(x, y). vdot x y) ([1.,2.], [3.,4.])
> ([3., 4.], [1., 2.])

:p
  f : Float -> Float = \x.
    y = x * 2.0
    yieldAccum (AddMonoid Float) \a.
      a += x * 2.0
      a += y
  grad f 1.0
> 4.

:p
  f : Float -> Float = \x.
    x2 = x * x
    with_reader x \xr.
      with_reader x2 \x2r.
        5.0 * (ask x2r) + 4.0 * (ask xr) + 2.0
  check_deriv f 2.0
> True

:p
  f : Float -> Float = \x.
    yield_state x \xr.
      for i:(Fin 2).
        xr := get xr * get xr
  check_deriv f 2.0
> True

:p
  f = \rec.
    ({x=x, y=y, z=z}) = rec
    x * y * i_to_f z
  (check_deriv (\x. f {x=x, y=4.0, z=5}) 2.0, check_deriv (\y. f {x=2.0, y=y, z=5}) 4.0)
> (True, True)

-- TODO: Re-enable once the big PR is merged
-- :p
--   f = \x.
--     y = for i:(Fin 10). { x=x * (IToF $ ordinal i) }
--     z = for i.
--       ({ x=x, ... }) = y.i
--       x
--     sum' z
--   checkDeriv f 1.0
-- > True

:p
  f = \x. for i:(Fin 4). { x=x * x * (i_to_f $ ordinal i) }
  jvp f 2.0 1.0
> [{x = 0.}, {x = 4.}, {x = 8.}, {x = 12.}]

:p
  s : { a : Float | b : Float } = case 2 == 2 of
    False -> {| a=2.0 |}
    True -> {| b=5.0 |}
  f = \x.
    case s of
      {| a=a |} -> 2.0 * (a + x)
      {| b=b |} -> b * x
  (y, df) = linearize f 2.0
  check_deriv_base f 2.0
> True

:p
  f = max 0.0
  (check_deriv_base f 1.0, check_deriv_base f (-1.0))
> (True, True)

:p
  xs = for i:(Fin 2). 2.0
  f = \x. sum xs
  check_deriv f 1.0
> True

:p
  f = \c.
    v = for i:(Fin 2). 2.0
    (c, v) = yield_state (c, v) \r. for i:(Fin 2).
      (c, v) = get r
      r := (c + sum v, v)
    c
  check_deriv f 1.0
> True

-- Test reference indexing
:p
  f = \x.
    i = unsafe_from_ordinal (Fin 3) 0
    mat = yield_state zero \m. m!i!i := x
    mat.i.i

  check_deriv f 1.0
> True

-- Regression test for bug #841, linearization through triangular table
func = \x:Float.
  table = for i:(Fin 2). for j:(..i). x
  tmp = (0 @ (Fin 2))
  sum table.tmp

snd (linearize func 1.0) 2.0
> 2.

-- Nested AD, examples from #713
def func2 {m} (x:m=>Float) : Float =
    exp (0.5 * sum for i. sq x.i)

def hvpf {m} (x:m=>Float) (v:m=>Float) : m=>Float =
    (dot x v) .* (grad func2 x) + func2(x) .* v

x = [0.1, 0.2, 0.3]
v = [0.2, 0.3, 0.4]
func2 x ~~ 1.072508
> True

-- analytic
result = hvpf x v
result ~~ [0.235952, 0.364653, 0.493354]
> True

-- finite diff over reverse
eps = 0.0001
(grad func2 (x + eps .* v) - grad func2 x) / eps ~~ [0.235885, 0.364631, 0.493228]
> True

-- reverse over forward
grad (\x. jvp func2 x v) x ~~ result
> True

-- reverse over reverse
grad (\x. dot (grad func2 x) v) x ~~ result
> True

-- forward over reverse
jvp (grad func2) x v ~~ result
> True

-- Regression test for bug #848, AD through state effect over case
def min' (a:Float) (b:Float) : Float =
  yield_state a \s.
    best = get s
    new_best = select (best < b) best 2.0
    s := new_best

grad (\x . (min' x (x+1))) 1.0
> 1.

grad (\x . [x].(argmin [x])) 1.0
> 1.
